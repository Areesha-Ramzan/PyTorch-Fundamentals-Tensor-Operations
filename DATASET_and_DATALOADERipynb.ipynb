{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a Simple Neural Network with nn.Module"
      ],
      "metadata": {
        "id": "WQ-cqcz6ov2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-j_HBcKSdJkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdCA8m8iR_Fh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Custom Neural Network Class\n",
        "class MySimpleNN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(num_features, 16),  # First hidden layer\n",
        "            nn.ReLU(),                    # Activation function\n",
        "            nn.Linear(16, 2)              # Output layer (for binary classification)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first created a custom neural network class by inheriting from nn.Module.\n",
        "\n",
        "Inside the constructor __init__, I added two fully connected layers (nn.Linear). The first one transforms the input features to a hidden layer, and the second one maps from that hidden layer to the final output layer.\n",
        "\n",
        "I also added a ReLU activation function in between the layers to introduce non-linearity into the model\n",
        "\n",
        "Then, I implemented the forward() method, which defines how the input data flows through the network layers during the forward pass."
      ],
      "metadata": {
        "id": "MY8od3aCo548"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random input data\n",
        "X = torch.rand(100, 5)                # 100 samples, 5 features\n",
        "y = torch.randint(0, 2, (100,))       # 100 binary labels (0 or 1)\n"
      ],
      "metadata": {
        "id": "k_x98tI0dIek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model instance\n",
        "model = MySimpleNN(num_features=5)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()               # suitable for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ],
      "metadata": {
        "id": "qpXpEoQdgF0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss\n",
        "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
        "        print(f'Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hrj-u1vgOIE",
        "outputId": "fb5e5c32-5ec2-4ad4-9787-5cf7aeb49b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6810\n",
            "Loss: 0.6804\n",
            "Loss: 0.6798\n",
            "Loss: 0.6794\n",
            "Loss: 0.6790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: Get predictions\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy = (predicted == y).sum().item() / y.size(0)\n",
        "\n",
        "print(f'Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNAgK3dEgZkC",
        "outputId": "89c34a39-4ba9-4ff0-f131-15063a6e8ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 58.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using a real-world dataset like MNIST I created a random dataset\n",
        "\n",
        "I converted this dataset into PyTorch tensors so it could be used with the model.\n",
        "\n",
        "For the loss function, I used CrossEntropyLoss, which is suitable for classification tasks.\n",
        "\n",
        "For optimization, I used Stochastic Gradient Descent (SGD) through torch.optim.SGD, and trained the model using a loop that runs for several epochs.\n",
        "\n",
        "During each epoch, I passed data through the model, calculated the loss, performed backpropagation, and updated the weights."
      ],
      "metadata": {
        "id": "CTOey9NtpMIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Dataset and DataLoader"
      ],
      "metadata": {
        "id": "rmNfvbgVhMNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import make_classification"
      ],
      "metadata": {
        "id": "bHOXE_wchASx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=make_classification(\n",
        "    n_samples=10,\n",
        "    n_features=2,\n",
        "    n_classes=2,\n",
        "    random_state=42,\n",
        "    n_informative=2,    # Number of informative features\n",
        "    n_redundant=0,      # Number of redundant features\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "IIn_Rm6QjO5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCNupoe3kcNU",
        "outputId": "2a3afea7-6fae-4e2d-cdf2-b6f0a5b6ad97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0683, -0.9701],\n",
              "        [-1.1402, -0.8388],\n",
              "        [-2.8954,  1.9769],\n",
              "        [-0.7206, -0.9606],\n",
              "        [-1.9629, -0.9923],\n",
              "        [-0.9382, -0.5430],\n",
              "        [ 1.7273, -1.1858],\n",
              "        [ 1.7774,  1.5116],\n",
              "        [ 1.8997,  0.8344],\n",
              "        [-0.5872, -1.9717]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zXBL6HXkdHs",
        "outputId": "4049049f-7eb9-44ee-a877-738cbfad43f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "LXUhwwRqj-Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "_1wDwoJehgPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " I defined a class called CustomDataset that inherited from torch.utils.data.Dataset.\n",
        "\n",
        "Inside this class, I implemented the __init__ method to accept the features and labels, and the __len__ method to return the size of the dataset.\n",
        "\n",
        "I implemented the __getitem__() method to return a single sample and its label when given an index."
      ],
      "metadata": {
        "id": "77J7QVCNpaK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=customDataset(X,y)"
      ],
      "metadata": {
        "id": "r79eLVyBiemp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO0LpHhaiklb",
        "outputId": "eecc11c8-a79c-4e9c-9420-816b0b38beb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader=DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "SDWYB1GDkjPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features, batch_labels in dataloader:\n",
        "  print(batch_features.shape)\n",
        "  print(batch_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x18I2wTzlbxm",
        "outputId": "69b491e7-31f8-41a9-a49a-549010e8516a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 2])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the custom dataset object, I used DataLoader to wrap it. I set batch_size=32 and shuffle=True so that data is randomly batched during training. Then, I tested it by looping through the DataLoader and printing the shape of a few batches. This confirmed that batching and shuffling were working correctly."
      ],
      "metadata": {
        "id": "LSIdFfzvpuFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 80% train, 20% test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n"
      ],
      "metadata": {
        "id": "zLf0fuLgmTVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "T3iBEvz2nm6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "MyynSz95nqEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for features, labels in train_loader:\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\" Loss: {total_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQtmCyY-nsJo",
        "outputId": "183b5da6-2e9a-4976-faf5-c2e293c03a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loss: 0.7230\n",
            " Loss: 0.7147\n",
            " Loss: 0.7066\n",
            " Loss: 0.6989\n",
            " Loss: 0.6915\n",
            " Loss: 0.6842\n",
            " Loss: 0.6772\n",
            " Loss: 0.6704\n",
            " Loss: 0.6638\n",
            " Loss: 0.6575\n",
            " Loss: 0.6513\n",
            " Loss: 0.6453\n",
            " Loss: 0.6394\n",
            " Loss: 0.6338\n",
            " Loss: 0.6283\n",
            " Loss: 0.6229\n",
            " Loss: 0.6177\n",
            " Loss: 0.6126\n",
            " Loss: 0.6077\n",
            " Loss: 0.6029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    labels=[0, 1],                         # Explicit class labels\n",
        "    target_names=[\"Class 0\", \"Class 1\"],  # Same order\n",
        "    zero_division=0                       # Avoid divide-by-zero warning\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRQk0XWXmv8-",
        "outputId": "07c9b561-084d-4e74-c599-2221442aa0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       1.00      1.00      1.00         2\n",
            "     Class 1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       0.50      0.50      0.50         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model and setting up the data pipeline, I moved on to evaluating the model a\n",
        "\n",
        "First, I split the dataset into training and testing sets using random_split.\n",
        "\n",
        "I used the trained model to predict labels on the test set, and compared these predictions with the true labels to calculate accuracy.\n",
        "\n",
        "To evaluate the model in more depth, I used classification_report from sklearn, which gives additional metrics like precision, recall, and F1 score.\n",
        "\n",
        "However, since the dataset was small, I faced an issue where sometimes the test set had only one class. I solved this by explicitly setting labels=[0, 1] inside classification_report() so it wouldnâ€™t crash."
      ],
      "metadata": {
        "id": "EetRJq9ep4Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "r7cZSH0ZqUCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried different learning rates (like 0.01 and 0.001) by changing the optim\n",
        "izer settings. I observed how a smaller learning rate made the model learn slowly but more precisely, while a bigger learning rate made it faster but risked overshooting.\n",
        "\n",
        "I also experimented with different batch sizes like 32 and 64 to see how it affected convergence.\n",
        "\n",
        "\n",
        "Finally, I tried training for different numbers of epochs (e.g., 10, 20, 30) to observe the loss curve and accuracy trends over time."
      ],
      "metadata": {
        "id": "_JoAXsamqG3s"
      }
    }
  ]
}